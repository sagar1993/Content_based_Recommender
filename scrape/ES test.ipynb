{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run/ check Elastic search\n",
    "\n",
    "    systemctl start elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"u4iVhDd\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"biTULKRYTPKW62ewYH9Q9g\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"6.4.2\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"deb\",\n",
      "    \"build_hash\" : \"04711c2\",\n",
      "    \"build_date\" : \"2018-09-26T13:34:09.098244Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"7.4.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"5.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get('http://localhost:9200')\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read dataframes and import data to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = pd.read_csv('oracleTM.csv', index_col=0)\n",
    "oracle.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikibooks = pd.read_csv('data.csv', index_col=0)\n",
    "wikibooks.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = oracle.append(wikibooks)\n",
    "# df = wikibooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply text pre-processing and create a dunction to apply for test data\n",
    "    lowercase, stemming, stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop = text.ENGLISH_STOP_WORDS\n",
    "porter_stemmer = PorterStemmer()\n",
    "# df['text'] = df[\"text\"].apply(lambda x: ''.join([\"\" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n",
    "\n",
    "wikibooks['text'] = wikibooks[\"text\"].apply(lambda x: ''.join([\"\" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n",
    "\n",
    "oracle['text'] = oracle[\"text\"].apply(lambda x: ''.join([\"\" if ord(i) < 32 or ord(i) > 126 else i for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(lambda x: ' '.join([porter_stemmer.stem(word.lower()) for word in x.split() if word not in (stop)]))\n",
    "\n",
    "wikibooks['text'] = wikibooks['text'].apply(lambda x: ' '.join([porter_stemmer.stem(word.lower()) for word in x.split() if word not in (stop)]))\n",
    "\n",
    "oracle['text'] = oracle['text'].apply(lambda x: ' '.join([porter_stemmer.stem(word.lower()) for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "    \n",
    "def preprocessing(df_col):\n",
    "    stop = text.ENGLISH_STOP_WORDS\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    df_col = df_col.apply(lambda x: ''.join([\"\" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n",
    "    df_col = df_col.apply(lambda x: ' '.join([porter_stemmer.stem(word.lower()) for word in x.split() if word not in (stop)]))\n",
    "    return df_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert records to elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['json'] = df.apply(lambda x: x.to_json(), axis=1)\n",
    "\n",
    "wikibooks['json'] = wikibooks.apply(lambda x: x.to_json(), axis=1)\n",
    "\n",
    "oracle['json'] = oracle.apply(lambda x: x.to_json(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nes.indices.delete(index='java', ignore=[400, 404])\\nfor index in range(len(df)):\\n    es.index(index='java', doc_type='document', id=index, body=df.iloc[index]['json'])\\n    \\nfor index in range(len(oracle)):\\n    es.index(index='java_oracle', doc_type='document', id=index, body=oracle.iloc[index]['json'])\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='java_oracle', ignore=[400, 404])\n",
    "#for index in range(len(oracle)):\n",
    "#    es.index(index='java_oracle', doc_type='document', id=index, body=oracle.iloc[index]['json'])\n",
    "\n",
    "es.indices.delete(index='java_wikibook', ignore=[400, 404])\n",
    "\n",
    "for index in range(len(wikibooks)):\n",
    "    es.index(index='java_wikibook', doc_type='document', id=index, body=wikibooks.iloc[index]['json'])\n",
    "\n",
    "\"\"\"\n",
    "es.indices.delete(index='java', ignore=[400, 404])\n",
    "for index in range(len(df)):\n",
    "    es.index(index='java', doc_type='document', id=index, body=df.iloc[index]['json'])\n",
    "    \n",
    "for index in range(len(oracle)):\n",
    "    es.index(index='java_oracle', doc_type='document', id=index, body=oracle.iloc[index]['json'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_excel('queries.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries['text_1'] = preprocessing(queries['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "ret = []\n",
    "\n",
    "for index in range(len(queries)):\n",
    "    #query = queries.iloc[index]['text_1']\n",
    "    query = queries.iloc[index]['text']\n",
    "    query_text = queries.iloc[index]['text']\n",
    "    \n",
    "    res_wikibooks = es.search(index='java_wikibook',from_=0, size=10, body={\"query\": {\n",
    "        \"multi_match\" : {\n",
    "          \"query\":    query,\n",
    "          \"fields\": [ \"title\"] \n",
    "        }\n",
    "    }})\n",
    "    \n",
    "    result = {}\n",
    "    result['data'] = []\n",
    "    \n",
    "    for item in res_wikibooks['hits']['hits']:\n",
    "        d = {}\n",
    "        title = item['_source']['title']\n",
    "        url = item['_source']['url']\n",
    "        d['title'] = title\n",
    "        d['url'] = url\n",
    "        result['data'].append(d)\n",
    "        result['title'] = query_text\n",
    "    ret.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('file.json', 'w') as outfile:\n",
    "    json.dump(ret, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import pandas as pd\n",
    "ret = []\n",
    "\n",
    "queries = pd.read_excel('queries.xlsx')\n",
    "\n",
    "for index in range(len(queries)):\n",
    "    query = queries.iloc[index]['text']\n",
    "    query_text = queries.iloc[index]['text']\n",
    "    \n",
    "    res_wikibooks = es.search(index='java_wikibook',from_=0, size=10, body={\"query\": {\n",
    "        \"multi_match\" : {\n",
    "          \"query\":    query,\n",
    "          \"fields\": [ \"title\"] \n",
    "        }\n",
    "    }})\n",
    "    \n",
    "    result = {}\n",
    "    result['data'] = []\n",
    "    \n",
    "    for item in res_wikibooks['hits']['hits']:\n",
    "        d = {}\n",
    "        title = item['_source']['title']\n",
    "        url = item['_source']['url']\n",
    "        d['title'] = title\n",
    "        d['url'] = url\n",
    "        result['data'].append(d)\n",
    "        result['title'] = query_text\n",
    "    ret.append(result)\n",
    "import json\n",
    "with open('file.json', 'w') as outfile:\n",
    "    json.dump(ret, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
